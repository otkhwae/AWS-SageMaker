{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EC2 - Elastic Compute Clous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H8> Virtualization\n",
    "\n",
    "long time ago a server used to host only one application but as they became more powerful with more cpu cores, more memory they could host multiple applications on same server.\n",
    "\n",
    "a hypervisor used to create multiple or virtual computers in the server each for a single application.\n",
    "each virtual machine is separate from each other and it runs its own operating system, has its own application and application system dependencies. and they cant talk to each other unless you want them to, through networking. so they are secure and isolated, whatever happens in the first virtual machine, itll not affect other virtual machines\n",
    "\n",
    "basically hypervisor chops us physical servers into a logical servers or virtual machines\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC2 basically is renting a server in the cloud instead of buying physical hardware ie gpus, cpus... to train your ml model. aws will take care of configuration for you."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H8> EC2 Instance types\n",
    "- standard\n",
    "- memory optimized\n",
    "- compute optimized\n",
    "- accelerated computing\n",
    "\n",
    "aws provides a selection of instance types optimised for different ml roles. this instances compromise of varying combinations of CPUs, GPUs, RAM and network capacity eg for standard: ml.t2.medium"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. stanadard type\n",
    "- these are the \"t2\"s & \"m5d\"s eg ml.t2.large, ml.m5d.2xlarge\n",
    "- least powerful class\n",
    "- low performance\n",
    "\n",
    "2. memory optimised\n",
    "- are for algos tat require more memory to train.\n",
    "-  these are the \"r\"s eg ml.r5.12xlarge, ml.r5d.24xlarge\n",
    "\n",
    "3. compute optimised\n",
    "- these are the \"c\"s eg ml.c5.18xlarge, ml.c4.8xlarge\n",
    "\n",
    "4. accelerated computing\n",
    "- these are specifically for gpus for DL models\n",
    "- these are the \"p,g,inf\"s eg ml.p3.2xlarge, ml.g4dn.16xlarge, ml.inf1.24xlarge\n",
    "\n",
    "when you train model is sagemake rstudio you got to specify the type of instance you are using\n",
    "\n",
    "<H8> Inference acceleration\n",
    "\n",
    "- aws offers you to have an accelerator on top of an endpoint and youll be able to leverage the speed of this accelerators at a fraction of a cost of having a dedicated gpu for u\n",
    "- aws sagemaker offer elastic inference (EI)\n",
    "- EI speeds up throughput and devrease latency to get real time inferences on your DL models\n",
    "- after model training is done, you need to deploy model and create an endpoint, then you make inferences on that endpoint eg send an obj like an image to get prediction.\n",
    "- instead of using a dedicated gpu instance for your endpoint, you can use a standard cpu instance and attach an accelerator\n",
    "- they are the \"eia\"s eg eia2.xlarge\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <H8> Instance pricing\n",
    "\n",
    " 1. on demand\n",
    " - pay compute capacity by the hour\n",
    " - short term\n",
    " - scalable\n",
    "\n",
    "  2. spot instances\n",
    "- cheaper than on demand instance 0.9 cheaper\n",
    "- got to wait until they are not in-use\n",
    "\n",
    " 3. reserved\n",
    " - reserve instances up front eg 1-3 yrs\n",
    " - can get massive discounts\n",
    "\n",
    "4. dedicated host\n",
    "- physical ec2 server that is dedicated for use\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d067567a45b8f23f20893b96648ea0bdbfd0e7249a1e525a925501ea4bfa66c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
